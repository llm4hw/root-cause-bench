import csv
from openai import OpenAI
import re
import os
# import pandas as pd
import openai

BUGS_CSV_FILE = r"D:\UNSW\chip chat\newnewnew\error_list.csv"

BUGS_DIR = r"D:\UNSW\chip chat\newnewnew\runtime_errors"
LLM_BUG_RESPONSES_DIR = "llm_response"
PROMPT = r"D:\UNSW\chip chat\newnewnew\prompt"

DEEPSEEK_TOKEN = "sk-a01ffd2abe174fa3a752654409f76acf"

AI_MODEL = "deepseekR1"

# --- Prompt design ---
# SYSTEM_PROMPT = """You are a helpful assistant which debugs RTL and HDL code in Verilog and VHDL. Do not provide code in your answer. Explain what has gone wrong and why a bug is occurring, but do not attempt to fix the bug yourself."""

# --- Prompt design ---
SYSTEM_PROMPT = """You are a helpful assistant which debugs RTL and HDL code in Verilog and VHDL. Do not provide code in your answer. Explain what has gone wrong and why a bug is occurring, but do not attempt to fix the bug yourself."""


# The `USER_PROMPTS` variable in the provided Python code is a list that contains formatted strings
# used as prompts for the OpenAI API. These prompts are used to request explanations for bugs in RTL
# and HDL code in Verilog and VHDL. Each element in the `USER_PROMPTS` list is a formatted string that
# includes placeholders for error messages, code files, testbench code, and additional information
# related to the bug.
USER_PROMPTS = [
    """Error message: %s\n\nFull code file:```%s```\n\nTestbench Code:```%s```\n\nScenario:%s\n\nWhat is the bug, why is it occurring, and how can it be fixed?\n
    Note: this error occurred during the simulation of the test bench and code file below. It is an error message generated by the test bench itself. You should determine the root cause of the issue and explain the fix.
    """
]

def main():
    #load the OpenAI token
    # with open(OPENAI_TOKEN_FILE, 'r') as file:
    #     openai_token = file.read().strip()

    # client = OpenAI(api_key=OPENAI_TOKEN)
    # client.organization = "org-PhuZaSvz3L7YWrH38mnU5ybq"
    client = OpenAI(api_key=DEEPSEEK_TOKEN, base_url="https://api.deepseek.com") # modify in this line
    # client.organization = "org-PhuZaSvz3L7YWrH38mnU5ybq"

    #load the bugs
    with open(BUGS_CSV_FILE, 'r') as file:
        reader = csv.DictReader(file)
        bugs_data = [row for row in reader]

    #for each bug
    for bug in bugs_data:
        # ensure that the error messages are correct wrt line numbers
        if bug["Error Message"] == "No Error Message":
            continue
        ###
        ###
        # if bug["Bug ID"] == "1" or bug["Bug ID"] == "2" or bug["Bug ID"] == "3" or bug["Bug ID"] == "4":
        #     continue
        ###
        ###
        ###
        print(bug["Bug ID"], ": ", bug["Type of Bug"])

        #load the buggy file
        with open(f"{BUGS_DIR}/bug_{bug['Bug ID']}/rtl/{bug['File Name']}", 'r') as file:
            buggy_code = file.read()
        
        testbench_name = f"tb_top{bug['Bug ID']}_module.v"
        with open(f"{BUGS_DIR}/bug_{bug['Bug ID']}/testbench/{testbench_name}", 'r', encoding='utf-8') as file:
            testbench = file.read()
            match = None


        # content = [
        #     (bug["Error Message"], buggy_code, testbench),
        # ]

        content = [
            (bug["Error Message"], buggy_code, testbench, bug["Question"]),
        ]

        for j in range(len(USER_PROMPTS)):
            # output a txt file with all errors and easy to compare
            # using the prompt to as sevel times
            for i in range(5):
                # check if the file have been check or not
                if os.path.exists(f"{BUGS_DIR}/bug_{bug['Bug ID']}/{LLM_BUG_RESPONSES_DIR}/bug_{bug['Bug ID']}_llm_{AI_MODEL}_iteration_{i}.txt"):
                    continue
                # get prompt index
                # prompt_index = USER_PROMPTS.index(prompt)

                #send the messages to the OpenAI API
                response = client.chat.completions.create(
                    model= "deepseek-reasoner",
                    # messages=
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},  # Use 'user' instead of 'system' for the prompt
                        {"role": "user", "content": USER_PROMPTS[j] % content[j]}
                    ],
                    n = 1
                )

                #print the response
                # print(response.choices[0].message.content)
                #save the response
                with open(f"{BUGS_DIR}/bug_{bug['Bug ID']}/{LLM_BUG_RESPONSES_DIR}/bug_{bug['Bug ID']}_llm_{AI_MODEL}_iteration_{i}.txt", 'w', encoding="utf-8") as file:
                    file.write(str(response.choices[0].message.content))

                # with open(f"{LLM_BUG_SUMMARY_DIR}/bug_{bug['Bug ID']}_ide_{bug['IDE']}_prompt_{PROMPT_STRATEGY[j]}_llm_{AI_MODEL}.txt", 'a', encoding="utf-8") as file:
                #     # Write new content to the file
                #     file.write(f"{str(response.choices[0].message.content)} \n")
                #     file.write("\n")
    
if __name__ == "__main__":
    main()